{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXlF6QkBFruO3iPerex2Bl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumpaten/masters-thesis-code/blob/main/Interbrand_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install BeautifulSoup\n",
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "oHm5i4DYuiyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json\n",
        "import html\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the filename of your text file containing the HTML content. This assumes that you copy pasted the html content from the website\n",
        "# https://interbrand.com/best-global-brands/apple/ and its element div.m06__chart into a txt file for further processing.\n",
        "# Adjust the file path if necessary.\n",
        "file_path = \"/content/drive/MyDrive/Interbrand_HTML_content.txt\"\n",
        "\n",
        "# Read the HTML content from the file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "div_tag = soup.find(\"div\", class_=\"m06__chart\")\n",
        "if div_tag is None:\n",
        "    raise ValueError(\"No <div> with class 'm06__chart' found in the file.\")\n",
        "\n",
        "# Extract and unescape the JSON string from the 'data-brands' attribute\n",
        "data_brands_str = div_tag.get(\"data-brands\")\n",
        "data_brands_unescaped = html.unescape(data_brands_str)\n",
        "\n",
        "# Convert the unescaped JSON string to a Python dictionary\n",
        "data_dict = json.loads(data_brands_unescaped)\n",
        "\n",
        "# Define the companies to filter\n",
        "companies_to_filter = [\n",
        "    \"Apple\", \"Microsoft\", \"Facebook\", \"Google\", \"Oracle\",\n",
        "    \"Cisco\", \"Salesforce\", \"SAP\", \"Accenture\", \"IBM\"\n",
        "]\n",
        "\n",
        "# Define the year range filter (inclusive)\n",
        "start_year = 2008\n",
        "end_year = 2021\n",
        "\n",
        "# Prepare a list to store the filtered records\n",
        "records = []\n",
        "\n",
        "# Loop through each company in the data\n",
        "for company, info in data_dict.items():\n",
        "    # Only process companies from our filter list\n",
        "    if company not in companies_to_filter:\n",
        "        continue\n",
        "\n",
        "    years = info.get(\"study_years\", [])\n",
        "    values = info.get(\"brand_values\", [])\n",
        "\n",
        "    # Loop over the paired years and values\n",
        "    for year, value in zip(years, values):\n",
        "        # Only keep the records for years between 2008 and 2021\n",
        "        if start_year <= year <= end_year:\n",
        "            # Convert \"null\" strings to Python None\n",
        "            if isinstance(value, str) and value.lower() == \"null\":\n",
        "                value = None\n",
        "            records.append({\n",
        "                \"Company\": company,\n",
        "                \"Year\": year,\n",
        "                \"Interbrand_Value\": value\n",
        "            })\n",
        "\n",
        "# Create a DataFrame from the records\n",
        "df = pd.DataFrame(records)\n",
        "print(df)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_path = '/content/drive/My Drive/Interbrand_data_filtered.csv'\n",
        "df.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "a7ykUj62lRgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "until here was the code for the web scraper, now it is the data manipulation that was done directly on the CSV file"
      ],
      "metadata": {
        "id": "_9_ipvAJ6qK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your filtered CSV file on Google Drive.\n",
        "csv_path = '/content/drive/My Drive/Interbrand_data_filtered.csv'\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# -------------------------------\n",
        "# For Oracle, add the known 2023 value first.\n",
        "# According to the source data, Oracle's brand value for 2023 is 34622.\n",
        "# This extra row will serve as an anchor to interpolate the missing 2020 and 2021 values.\n",
        "oracle_extra = pd.DataFrame({\n",
        "    'Company': ['Oracle'],\n",
        "    'Year': [2023],\n",
        "    'Interbrand_Value': [34622]\n",
        "})\n",
        "df = pd.concat([df, oracle_extra], ignore_index=True)\n",
        "\n",
        "# Now, isolate Oracle's data and sort by Year.\n",
        "mask_oracle = df['Company'] == 'Oracle'\n",
        "df_oracle = df[mask_oracle].sort_values('Year')\n",
        "\n",
        "print(\"Oracle data BEFORE interpolation:\")\n",
        "print(df_oracle)\n",
        "\n",
        "# Perform linear interpolation on Oracle's Value column.\n",
        "# limit_direction='both' ensures that missing values at either end (if any) are also filled.\n",
        "df_oracle['Interbrand_Value'] = df_oracle['Interbrand_Value'].interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "print(\"\\nOracle data AFTER interpolation:\")\n",
        "print(df_oracle)\n",
        "\n",
        "# Replace the Oracle rows in the main DataFrame with the interpolated values.\n",
        "# (Sorting again by Year ensures correct assignment.)\n",
        "df.loc[mask_oracle, 'Interbrand_Value'] = df_oracle.sort_values('Year')['Interbrand_Value']\n",
        "\n",
        "# Since your dataset should only span 2008 to 2021, remove the extra 2023 row.\n",
        "df = df[~((df['Company'] == 'Oracle') & (df['Year'] == 2023))]\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file on your Google Drive.\n",
        "output_path = '/content/drive/My Drive/Interbrand_data_filtered_oracle.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "print(\"\\nUpdated data saved to:\", output_path)"
      ],
      "metadata": {
        "id": "bfPTpNfPtP5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to your filtered oracle CSV file on Google Drive.\n",
        "# Here we assume the file already contains Oracleâ€™s interpolated values\n",
        "# and earlier imputation for Salesforce (if any).\n",
        "csv_path = '/content/drive/My Drive/Interbrand_data_filtered_oracle.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Convert 'Year' and 'Value' to numeric.\n",
        "df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
        "df['Interbrand_Value'] = pd.to_numeric(df['Interbrand_Value'], errors='coerce')\n",
        "\n",
        "# Ensure the imputation indicator column exists (if not, initialize to 0).\n",
        "if 'is_imputed' not in df.columns:\n",
        "    df['is_imputed'] = 0\n",
        "\n",
        "# List companies that had a delayed debut (i.e., their first observed value is well after 2008)\n",
        "# For these companies, we will set pre-debut years to 0.\n",
        "companies_with_delayed_IPO = ['Facebook', 'Salesforce']\n",
        "\n",
        "for company in companies_with_delayed_IPO:\n",
        "    mask = df['Company'] == company\n",
        "    # Find the first year with an observed (non-NA) value.\n",
        "    debut_year = df.loc[mask & df['Interbrand_Value'].notna(), 'Year'].min()\n",
        "    # For years before the debut year, set the Value to 0.\n",
        "    df.loc[mask & (df['Year'] < debut_year), 'Interbrand_Value'] = 0\n",
        "    # Mark these rows as imputed.\n",
        "    df.loc[mask & (df['Year'] < debut_year), 'is_imputed'] = 1\n",
        "\n",
        "# (For Oracle, we leave the imputation indicator as 0 because the interpolated values are trusted.)\n",
        "\n",
        "\n",
        "# Print a summary for each company to verify\n",
        "for company in companies_with_delayed_IPO:\n",
        "    print(f\"\\n{company} data after adjusting pre-debut values:\")\n",
        "    print(df[df['Company'] == company].sort_values('Year'))\n",
        "\n",
        "# Save the final dataset with consistent imputation to a new CSV file.\n",
        "output_path = '/content/drive/My Drive/Interbrand_data_filtered_complete.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "print(\"\\nFinal complete dataset saved to:\", output_path)\n"
      ],
      "metadata": {
        "id": "8FG2rd7pujXZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}