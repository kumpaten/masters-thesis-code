{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNidO0y9QRVl89VUpXVbsMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumpaten/masters-thesis-code/blob/main/USPTO_patents_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The SQL query used to retrieve the raw patent data found in \"/content/drive/MyDrive/bq-results-20250212-194745-1739389687161/bq-results-20250212-194745-1739389687161.csv\" in the next step\n",
        "\"\"\"\n",
        "SELECT\n",
        "    p.date,\n",
        "    p.id,\n",
        "    p.num_claims,\n",
        "    a.assignee_id,\n",
        "    a.type,\n",
        "    a.organization\n",
        "  FROM\n",
        "    `patents-public-data.patentsview.patent` AS p\n",
        "  JOIN\n",
        "    `patents-public-data.patentsview.rawassignee` AS a\n",
        "      ON p.id = a.patent_id\n",
        "  WHERE\n",
        "    p.date BETWEEN '2008-01-01' AND '2021-12-31'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aQiJMKhvPznY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Filter Organization Names for Each Company and Aggregate num_claims by Year\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Step 1: Mount Google Drive and load the CSV file ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update the file path as necessary\n",
        "file_path = '/content/drive/MyDrive/bq-results-20250212-194745-1739389687161/bq-results-20250212-194745-1739389687161.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure required columns exist: \"organization\", \"patent_date\", \"num_claims\"\n",
        "for col in [\"organization\", \"date\", \"num_claims\"]:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"The CSV file must have a '{col}' column.\")\n",
        "\n",
        "# Ensure all organization values are strings (fill missing values with empty string)\n",
        "df[\"organization\"] = df[\"organization\"].fillna(\"\").astype(str)\n",
        "\n",
        "# Convert \"patent_date\" to datetime and extract the year\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "df[\"year\"] = df[\"date\"].dt.year\n",
        "\n",
        "# --- Step 2: Define filtering rules for each company ---\n",
        "# For each company we define:\n",
        "#   - \"allowed\": a list of keywords that must be present (case-insensitive)\n",
        "#   - \"forbidden\": a list of keywords that if present (case-insensitive) indicate the name is not genuine\n",
        "company_filters = {\n",
        "    \"microsoft\": {\n",
        "         \"allowed\": [\"microsoft\"],\n",
        "         \"forbidden\": [\"orthopedics\", \"hitachi\", \"tangis\", \"systems inc.\"]\n",
        "    },\n",
        "    \"apple\": {\n",
        "         \"allowed\": [\"apple\"],\n",
        "         \"forbidden\": [ \"applera\",         # different company\n",
        "                        \"appleton\",        # e.g., Appleton Papers, Appleton Grp, etc.\n",
        "                        \"snapple\",         # beverage companies\n",
        "                        \"dapple\",          # Dapple Equine Products\n",
        "                        \"kappler\",         # Kappler-related names\n",
        "                        \"grapplers\",       # Grapplers, Inc.\n",
        "                        \"three apples\",    # Three Apples Cosmetics\n",
        "                        \"zapple\",          # ZAPPLE, INC.\n",
        "                        \"pineapple\",       # Project Pineapple, Pineapple Express\n",
        "                        \"applejack\",       # Applejack 199 L.P.\n",
        "                        \"inferno\",         # Apple Inferno, Inc.\n",
        "                        \"biomedical\",      # Apple Biomedical*\n",
        "                        \"improvement\",     # Midwest Apple Improvement Association\n",
        "                        \"project pineapple\",  # Project Pineapple, LLC\n",
        "                        \"shanghai\",        # Shanghai Apple Flavor & Fragrance Co.\n",
        "                        \"ju studio\",       # Apple Ju Studio Inc.\n",
        "                        \"applegate\",       # Applegate Livestock Equipment, Inc.\n",
        "                        \"partners\",        # Apple Partners, LP\n",
        "                        \"applesauce\",      # Applesauce Project Inc.\n",
        "                        \"little green\",    # Little Green Apples, Inc.\n",
        "                        \"wapple\",          # Wapple.net Ltd\n",
        "                        \"applexion\",       # Applexion\n",
        "                        \"electric car\",    # Apple Electric Car, Inc.\n",
        "                        \"ii 'c\",          # APPLE II 'C.\n",
        "                        \"appled\",         # Appled Materials, Inc.\n",
        "                        \"apples-to-go\",\n",
        "                        \"appleid materials, inc\",\n",
        "                        \"appleboy\",\n",
        "                        \"mayapple\",\n",
        "                        \"candi\"]\n",
        "    },\n",
        "    \"salesforce\": {\n",
        "         \"allowed\": [\"salesforce\"],\n",
        "         \"forbidden\": []\n",
        "    },\n",
        "    \"facebook\": {\n",
        "         \"allowed\": [\"facebook\"],\n",
        "         \"forbidden\": []\n",
        "    },\n",
        "    # For SAP, we will later use a stricter (whitelist) approach.\n",
        "    \"sap\": {\n",
        "         \"allowed\": [\"sap\"],\n",
        "         \"forbidden\": [\"sapphire\",       # e.g., Sapphire Energy, etc.\n",
        "                       \"sapporo\",        # e.g., Sapporo Breweries, etc.\n",
        "                       \"saplo\",          # e.g., Saplo AB\n",
        "                       \"sapling\",        # e.g., Sapling Company, The Sapling Company\n",
        "                       \"aisapack\",       # e.g., AISAPACK Holding S.A.\n",
        "                       \"asap\",           # e.g., ASAP BreatheAssist, etc.\n",
        "                       \"sapiens\",        # e.g., Sapiens Steering Brain Stimulation, etc.\n",
        "                       \"saphinov\",       # e.g., Saphinov S.N.C.\n",
        "                       \"extrus\",         # covers \"extrusions\", \"extrustions\", etc.\n",
        "                       \"sapere\",         # e.g., Sapere IP, LLC\n",
        "                       \"sapurast\",       # e.g., Sapurast Research LLC\n",
        "                       \"portals\",        # e.g., SAP Portals Israel Ltd, etc.\n",
        "                       \"ohio aerosapce\",  # Ohio Aerosapce Institute\n",
        "                       \"profiler\",       # e.g., Sapa Profiler AB\n",
        "                       \"sapdesign\",      # e.g., SAPDesign AS\n",
        "                       \"sapo u.s.a\",     # Sapo U.S.A. Corp.\n",
        "                       \"fricaeco\",       # e.g., Fricaeco America SAPI de C.V.\n",
        "                       \"sapient\",        # e.g., Sapience Corporation\n",
        "                       \"sapnsion\",       # e.g., Sapnsion LLC\n",
        "                       \"saphran\",        # e.g., Saphran Inc. / Saphran, Inc.\n",
        "                       \"sapiotec\",       # e.g., SAPIOTEC GMBH\n",
        "                       \"saphena\",        # e.g., Saphena Medical, Inc.\n",
        "                       \"sapheon\",        # e.g., Sapheon, Inc.\n",
        "                       \"sapag\",          # e.g., SAPAG\n",
        "                       \"sap link\",       # e.g., Sap Link Technology Corp.\n",
        "                       \"sapsa\",          # e.g., Sapsa Bedding S.R.L.\n",
        "                       \"sapturf\",        # e.g., Sapturf, LLC\n",
        "                       \"sapre\",         # e.g., SAPREX, LLC\n",
        "                       \"sappel\",         # e.g., Sappel\n",
        "                       \"saponaqua\",      # e.g., Saponaqua International Limited\n",
        "                       \"drinksapor\",     # e.g., DRINK SAPORÉ INC.\n",
        "                       \"sap markets\",    # e.g., Sapmarkets Inc.\n",
        "                       \"mesaplex\",       # e.g., Mesaplexx Pty Ltd, etc.\n",
        "                       \"isapac\",         # e.g., Isapac Participaçöes SA.\n",
        "                       \"universita\",     # academic institutions\n",
        "                       \"sap link\",\n",
        "                       \"whatsapp\",\n",
        "                       \"sapience\", \"yehuda\", \"sensaphonics\", \"heat\", \"sappi\", \"sapna\", \"sapa\", \"sapir\", \"guisapet\", \"saphinon\", \"drossapharm\", \"sapheco\", \"ursapharm\", \"konstantin\", \"saphire\", \"sensapex\", \"sapio\", \"sapir\", \"sapience\", \"sapienza\", \"visapa\", \"sapmarkets\", \"arisaph\", \"chesapeake\", \"crimsape\", \"misapplied\", \"sapiselco\", \"drink\", \"sensapex\", \"amsapplied\", \"sapp\", \"saprise\"]\n",
        "    },\n",
        "    \"google\": {\n",
        "         \"allowed\": [\"google\"],\n",
        "         \"forbidden\": []\n",
        "    },\n",
        "    \"oracle\": {\n",
        "         \"allowed\": [\"oracle\"],\n",
        "         \"forbidden\": []\n",
        "    },\n",
        "    \"cisco\": {\n",
        "         \"allowed\": [\"cisco\"],\n",
        "         \"forbidden\": [\"san francisco\", \"university\", \"fundacao\", \"certis\", \"photonics\", \"aldef global\"]\n",
        "    },\n",
        "    \"accenture\": {\n",
        "         \"allowed\": [\"accenture\"],\n",
        "         \"forbidden\": [\"properties\", \" corporation\", \" ans\"]\n",
        "    },\n",
        "    \"ibm\": {\n",
        "         \"allowed\": [\"ibm\", \"international business machine\", \"international-business\", \"i.b.m\", \"ibm corp\"],\n",
        "         \"forbidden\": [\"the libman\", \"instituto\", \"institute\", \"ibmt\", \"ibmv\", \"hibm\", \"ibmc\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Step 3: Filter organization names for each company ---\n",
        "results_dict = {}\n",
        "\n",
        "for company, rules in company_filters.items():\n",
        "\n",
        "    allowed_keywords = rules[\"allowed\"]\n",
        "    forbidden_keywords = rules[\"forbidden\"]\n",
        "    condition_allowed = df[\"organization\"].str.lower().apply(\n",
        "        lambda org: any(keyword in org for keyword in allowed_keywords)\n",
        "    )\n",
        "    condition_forbidden = df[\"organization\"].str.lower().apply(\n",
        "        lambda org: any(keyword in org for keyword in forbidden_keywords)\n",
        "    )\n",
        "    filtered_orgs = df[condition_allowed & (~condition_forbidden)][\"organization\"].unique().tolist()\n",
        "\n",
        "    results_dict[company] = filtered_orgs\n",
        "\n",
        "# --- Step 4: Aggregate num_claims by year for each company ---\n",
        "agg_results = {}\n",
        "\n",
        "for company, org_list in results_dict.items():\n",
        "    # Filter the DataFrame to rows where \"organization\" is in the filtered list for this company.\n",
        "    subset = df[df[\"organization\"].isin(org_list)]\n",
        "    # Group by \"year\" and sum \"num_claims\"\n",
        "    grouped = subset.groupby(\"year\")[\"num_claims\"].sum().reset_index()\n",
        "    grouped[\"company\"] = company  # add a company column for later pivoting\n",
        "    agg_results[company] = grouped\n",
        "\n",
        "# Combine all companies' aggregated results into one DataFrame.\n",
        "agg_df = pd.concat(agg_results.values(), ignore_index=True)\n",
        "\n",
        "# Create a pivot table: rows = year, columns = company, values = sum(num_claims)\n",
        "pivot_table = agg_df.pivot(index=\"year\", columns=\"company\", values=\"num_claims\").fillna(0)\n",
        "\n",
        "# --- Step 5: Print the pivot table ---\n",
        "print(\"Pivot Table: Aggregated num_claims by Year for Each Company\")\n",
        "print(pivot_table)\n"
      ],
      "metadata": {
        "id": "dxv_5ldJq3Ud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "698acbe4-9e20-4928-fa42-a813d413a570"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Pivot Table: Aggregated num_claims by Year for Each Company\n",
            "company  accenture    apple    cisco  facebook   google       ibm  microsoft  \\\n",
            "year                                                                           \n",
            "2008        1680.0   5798.0  19911.0       0.0   1650.0   47208.0    45413.0   \n",
            "2009        1107.0   8536.0  23975.0       0.0   3619.0   56806.0    58555.0   \n",
            "2010        2575.0  16090.0  27774.0     143.0   7317.0   91547.0    58039.0   \n",
            "2011        3011.0  17369.0  22907.0     315.0  11265.0  100387.0    42690.0   \n",
            "2012        2440.0  27101.0  20947.0     991.0  26281.0  107610.0    47428.0   \n",
            "2013        3148.0  40565.0  18412.0    2723.0  41346.0  113561.0    50194.0   \n",
            "2014        3726.0  44339.0  22184.0    6551.0  55049.0  122049.0    54108.0   \n",
            "2015        2510.0  41751.0  19267.0    8206.0  64758.0  112817.0    47213.0   \n",
            "2016        3159.0  44615.0  19070.0    9107.0  63891.0  112859.0    47479.0   \n",
            "2017        2964.0  48413.0  18744.0   13596.0  52726.0  124612.0    48208.0   \n",
            "2018        3857.0  47653.0  16316.0   12777.0  42461.0  129725.0    45492.0   \n",
            "2019        5378.0  55837.0  19889.0   25074.0  41832.0  140294.0    60150.0   \n",
            "2020        5769.0  61764.0  20370.0   28601.0  36055.0  148490.0    57018.0   \n",
            "2021        4637.0  44306.0  14736.0   17321.0  22817.0  107938.0    37136.0   \n",
            "\n",
            "company   oracle  salesforce      sap  \n",
            "year                                   \n",
            "2008      5677.0         0.0   5434.0  \n",
            "2009      6093.0       156.0   7765.0  \n",
            "2010     18167.0       598.0  12633.0  \n",
            "2011     18969.0       514.0   9426.0  \n",
            "2012     17794.0      1041.0   9993.0  \n",
            "2013     15808.0      2327.0  10276.0  \n",
            "2014     16313.0      3437.0  11321.0  \n",
            "2015     13841.0      3660.0  10454.0  \n",
            "2016     13544.0      4264.0   8934.0  \n",
            "2017     15158.0      4174.0   8204.0  \n",
            "2018     13773.0      3832.0   7087.0  \n",
            "2019     16839.0      6371.0  10745.0  \n",
            "2020     15083.0      8818.0  12615.0  \n",
            "2021      9364.0      7150.0   8407.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6: Save the pivot table to CSV and download it ---\n",
        "\n",
        "# Define output file path on your Google Drive\n",
        "output_csv_path = '/content/drive/MyDrive/aggregated_claims_by_year.csv'\n",
        "pivot_table.to_csv(output_csv_path)\n",
        "print(\"\\nCSV file saved to:\", output_csv_path)\n",
        "\n",
        "#(Optional) Download the CSV file to your local machine\n",
        "#from google.colab import files\n",
        "#files.download(output_csv_path)"
      ],
      "metadata": {
        "id": "drZcsILX1Kvb",
        "outputId": "6628fb1a-a06e-41b8-e9ab-1bea699c62dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CSV file saved to: /content/drive/MyDrive/aggregated_claims_by_year.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_81793904-f511-43e8-b30c-5833fbe3aa9d\", \"aggregated_claims_by_year.csv\", 1212)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}