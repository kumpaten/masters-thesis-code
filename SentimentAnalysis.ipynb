{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumpaten/masters-thesis-code/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIstLh-8b26N",
        "outputId": "cc271bf3-307f-42b3-b3fa-e0c4efc11440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install vaderSentiment tiktoken openai"
      ],
      "metadata": {
        "id": "VwpuVOJ2RWlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run GPT-4o on the 10-K (20-F) filings of each company and return sentiment scores for different aspects\n",
        "\n",
        "# — Imports & setup\n",
        "from google.colab import userdata\n",
        "import openai, tiktoken, os, time, pandas as pd, re, random, math\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from openai import RateLimitError, APIError, OpenAIError\n",
        "\n",
        "# — API client\n",
        "env_key = userdata.get('API_KEY_OPENAI')\n",
        "client = openai.OpenAI(api_key=env_key)\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "INTANGIBLE_ASPECTS = [\n",
        "    \"Employee Satisfaction Sentiment\",\n",
        "    \"Customer Satisfaction Sentiment\",\n",
        "    \"Innovation Sentiment\",\n",
        "    \"Intellectual Property Sentiment\",\n",
        "    \"Brand Strength Sentiment\",\n",
        "]\n",
        "\n",
        "# Keywords to extract relevant sentences for each aspect\n",
        "ASPECT_KEYWORDS = {\n",
        "    \"Employee Satisfaction Sentiment\": [\n",
        "        \"employee\", \"staff\", \"workforce\", \"morale\", \"engagement\", \"retention\",\n",
        "        \"turnover\", \"compensation\", \"benefits\", \"training\", \"development\",\n",
        "        \"culture\", \"team\", \"productivity\", \"well‐being\", \"leadership\", \"recognition\"\n",
        "    ],\n",
        "    \"Customer Satisfaction Sentiment\": [\n",
        "        \"customer\", \"client\", \"satisfaction\", \"nps\", \"net promoter\", \"service\",\n",
        "        \"support\", \"feedback\", \"loyalty\", \"experience\", \"complaint\", \"resolution\",\n",
        "        \"churn\", \"retention\", \"adoption\", \"recommendation\", \"acquisition\"\n",
        "    ],\n",
        "    \"Innovation Sentiment\": [\n",
        "        \"innov\", \"innovation\", \"research\", \"development\", \"r&d\", \"patent\",\n",
        "        \"breakthrough\", \"technology\", \"prototype\", \"creativity\", \"disruption\",\n",
        "        \"experiment\", \"iteration\", \"platform\", \"solution\", \"idea\"\n",
        "    ],\n",
        "    \"Intellectual Property Sentiment\": [\n",
        "        \"intellectual property\", \"ip\", \"patent\", \"trademark\", \"copyright\",\n",
        "        \"license\", \"infringement\", \"portfolio\", \"rightholder\", \"licensing\",\n",
        "        \"royalties\", \"trade secret\", \"proprietary\", \"filing\"\n",
        "    ],\n",
        "    \"Brand Strength Sentiment\": [\n",
        "        \"brand\", \"reputation\", \"marketing\", \"awareness\", \"equity\", \"image\",\n",
        "        \"positioning\", \"perception\", \"branding\", \"advertising\", \"pr\",\n",
        "        \"recognition\", \"visibility\", \"identity\", \"loyalty\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Paths\n",
        "BASE_DIR      = '/content/drive/MyDrive/10K-20F-Filings-Cleaned'\n",
        "OUTPUT_SCORES = '/content/drive/MyDrive/intangible_scores.csv'\n",
        "OUTPUT_JUSTS  = '/content/drive/MyDrive/intangible_justifications.csv'\n",
        "\n",
        "# — Chunking helper\n",
        "def chunk_text(text, max_tokens=190000, model=MODEL):\n",
        "    try:\n",
        "        enc = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    toks = enc.encode(text)\n",
        "    return [enc.decode(toks[i:i+max_tokens])\n",
        "            for i in range(0, len(toks), max_tokens)]\n",
        "\n",
        "# — Extract aspect-relevant sentences with improved filtering\n",
        "def extract_aspect_text(chunk, aspect):\n",
        "    kws = ASPECT_KEYWORDS[aspect]\n",
        "    sentences = re.split(r'(?<=[.!?]) +', chunk)\n",
        "    relevant_sents = []\n",
        "\n",
        "    # First pass - collect all sentences with exact keywords\n",
        "    for s in sentences:\n",
        "        # Only include sentences with keywords and minimum length\n",
        "        if len(s) > 30 and any(k in s.lower() for k in kws):\n",
        "            relevant_sents.append(s)\n",
        "\n",
        "    # If we have at least 15 relevant sentences (approx. 500-1000 words), use those\n",
        "    if len(relevant_sents) >= 15:\n",
        "        return \" \".join(relevant_sents)\n",
        "\n",
        "    # If we don't have enough direct matches, expand search to nearby sentences\n",
        "    # This captures context sentences that might not contain the keywords\n",
        "    if len(relevant_sents) >= 3:\n",
        "        expanded_sents = []\n",
        "        matched_indices = [sentences.index(s) for s in relevant_sents if s in sentences]\n",
        "\n",
        "        for idx in matched_indices:\n",
        "            # Include 1 sentence before and after each relevant sentence\n",
        "            start = max(0, idx - 1)\n",
        "            end = min(len(sentences), idx + 2)\n",
        "            expanded_sents.extend(sentences[start:end])\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        seen = set()\n",
        "        expanded_unique = [s for s in expanded_sents if not (s in seen or seen.add(s))]\n",
        "\n",
        "        if len(expanded_unique) >= 5:\n",
        "            return \" \".join(expanded_unique)\n",
        "\n",
        "    # Still not enough? Get a larger sample of the document\n",
        "    # Prioritize the beginning and end of the chunk where key info often appears\n",
        "    sample_size = min(150, len(sentences) // 2)\n",
        "    start_sentences = sentences[:sample_size]\n",
        "    end_sentences = sentences[-sample_size:] if len(sentences) > sample_size else []\n",
        "\n",
        "    return \" \".join(start_sentences + end_sentences)\n",
        "\n",
        "# — VADER sentiment analysis with bounded output\n",
        "def vader_delta(text):\n",
        "    if not text or len(text) < 50:\n",
        "        return 0.0\n",
        "\n",
        "    # Get compound score and convert to -3 to +3 range\n",
        "    score = detector.polarity_scores(text)['compound'] * 3.0\n",
        "\n",
        "    # Apply sigmoid to ensure extreme values are less common\n",
        "    return 2 * (1 / (1 + math.exp(-score))) - 1\n",
        "\n",
        "# Initialize VADER\n",
        "detector = SentimentIntensityAnalyzer()\n",
        "\n",
        "# — Single-aspect GPT call with robust float parsing\n",
        "def analyze_aspect_chunk(text, aspect, prev_score, prev_summary, retries=3):\n",
        "    if not text or len(text) < 100:\n",
        "        return 0.0, \"Insufficient relevant content to analyze.\"\n",
        "\n",
        "    # Construct context that includes previous year information\n",
        "    context = f\"\"\"\n",
        "Analyze the sentiment change for {aspect} in this company's 10-K filing compared to the previous year.\n",
        "\n",
        "PREVIOUS YEAR:\n",
        "- Score: {prev_score:.1f} (on scale 0-10)\n",
        "- Assessment: \"{prev_summary}\"\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Evaluate ONLY the sentiment change from last year to this year.\n",
        "2. Consider both explicit statements and tone regarding {aspect}.\n",
        "3. Note any significant positives or negatives mentioned.\n",
        "4. Compare the current language with the previous assessment.\n",
        "\n",
        "SCALE:\n",
        "- -2.0: Major negative change (significantly worse sentiment)\n",
        "- -1.0: Moderate negative change\n",
        "- 0.0: No significant change\n",
        "- +1.0: Moderate positive change\n",
        "- +2.0: Major positive change (significantly better sentiment)\n",
        "\n",
        "Your response MUST follow this format:\n",
        "<delta> || <detailed justification>\n",
        "\n",
        "The justification should be 3-5 sentences explaining your reasoning with specific examples from the text.\n",
        "\"\"\"\n",
        "\n",
        "    # Add a section with key aspect words to focus on\n",
        "    context += f\"\\nKEY FOCUS AREAS FOR {aspect.upper()}:\\n\"\n",
        "    context += \", \".join(ASPECT_KEYWORDS[aspect][:10])  # Include top keywords\n",
        "\n",
        "    # Complete the prompt with the text for analysis\n",
        "    prompt = context + f\"\\n\\n---\\nTHIS YEAR'S 10-K TEXT ABOUT {aspect.upper()}:\\n{text}\\n---\"\n",
        "\n",
        "    for i in range(1, retries+1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "                temperature=0.2,\n",
        "                max_tokens=1500\n",
        "            )\n",
        "            out = resp.choices[0].message.content.strip()\n",
        "            if '||' in out:\n",
        "                d_str, just = out.split('||',1)\n",
        "                m = re.search(r'-?\\d+\\.?\\d*', d_str)\n",
        "                if m:\n",
        "                    # Explicitly bound the delta to prevent extreme values\n",
        "                    delta = float(m.group(0))\n",
        "                    return max(-2.0, min(2.0, delta)), just.strip()\n",
        "        except RateLimitError:\n",
        "            wait = 5 * i\n",
        "            time.sleep(wait)\n",
        "        except (APIError, OpenAIError):\n",
        "            time.sleep(5)\n",
        "\n",
        "    # Skip if all retries fail\n",
        "    return 0.0, \"Analysis failed after multiple attempts.\"\n",
        "\n",
        "# Function to initialize baseline scores with differentiation\n",
        "def initialize_baseline_scores():\n",
        "    # Start with differentiated scores for each aspect\n",
        "    return {\n",
        "        \"Employee Satisfaction Sentiment\": 5.0 + random.uniform(-0.5, 0.5),\n",
        "        \"Customer Satisfaction Sentiment\": 5.0 + random.uniform(-0.5, 0.5),\n",
        "        \"Innovation Sentiment\": 5.0 + random.uniform(-0.5, 0.5),\n",
        "        \"Intellectual Property Sentiment\": 5.0 + random.uniform(-0.5, 0.5),\n",
        "        \"Brand Strength Sentiment\": 5.0 + random.uniform(-0.5, 0.5)\n",
        "    }\n",
        "\n",
        "# — Improved score calculation that allows for both increases and decreases\n",
        "def calculate_new_score(prev_score, delta):\n",
        "    # Scale factor decreases as scores move away from the middle (5.0)\n",
        "    distance_from_center = abs(prev_score - 5.0)\n",
        "    scale_factor = 1.0 - (distance_from_center / 10.0)\n",
        "\n",
        "    # Calculate the raw score change\n",
        "    # The farther from 5, the harder it is to move in that direction\n",
        "    if (delta > 0 and prev_score > 5.0) or (delta < 0 and prev_score < 5.0):\n",
        "        # Moving farther from center is harder\n",
        "        adjusted_delta = delta * scale_factor\n",
        "    else:\n",
        "        # Moving toward center is easier\n",
        "        adjusted_delta = delta * 1.0\n",
        "\n",
        "    # Calculate new score and ensure it stays within bounds\n",
        "    new_score = prev_score + adjusted_delta\n",
        "    return round(max(1.0, min(9.0, new_score)), 1)  # Limit to 1.0-9.0 range\n",
        "\n",
        "# Function to save a summary text file with key findings\n",
        "def save_summary_report(scores_rows, justs_rows, output_path):\n",
        "    companies = set(row['Company'] for row in scores_rows)\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(\"# Intangible Asset Sentiment Analysis Summary Report\\n\\n\")\n",
        "\n",
        "        for company in sorted(companies):\n",
        "            f.write(f\"## {company}\\n\\n\")\n",
        "\n",
        "            # Get all years for this company\n",
        "            company_scores = [row for row in scores_rows if row['Company'] == company]\n",
        "            company_years = sorted(set(row['Year'] for row in company_scores))\n",
        "\n",
        "            if not company_years:\n",
        "                f.write(\"No data available.\\n\\n\")\n",
        "                continue\n",
        "\n",
        "            # Create a summary of score trends\n",
        "            f.write(\"### Score Trends\\n\\n\")\n",
        "            f.write(\"| Year | \" + \" | \".join(INTANGIBLE_ASPECTS) + \" |\\n\")\n",
        "            f.write(\"|------|\" + \"|\".join([\"------|\" for _ in INTANGIBLE_ASPECTS]) + \"\\n\")\n",
        "\n",
        "            for year in company_years:\n",
        "                year_data = next((row for row in company_scores if row['Year'] == year), None)\n",
        "                if year_data:\n",
        "                    f.write(f\"| {year} | \")\n",
        "                    f.write(\" | \".join([f\"{year_data.get(asp, 'N/A'):.1f}\" for asp in INTANGIBLE_ASPECTS]))\n",
        "                    f.write(\" |\\n\")\n",
        "\n",
        "            f.write(\"\\n### Key Insights\\n\\n\")\n",
        "\n",
        "            # Find interesting trends\n",
        "            for asp in INTANGIBLE_ASPECTS:\n",
        "                asp_scores = [(row['Year'], row.get(asp, None)) for row in company_scores]\n",
        "                asp_scores = [(year, score) for year, score in asp_scores if score is not None]\n",
        "\n",
        "                if len(asp_scores) >= 2:\n",
        "                    asp_scores.sort()  # Sort by year\n",
        "                    first_year, first_score = asp_scores[0]\n",
        "                    last_year, last_score = asp_scores[-1]\n",
        "\n",
        "                    # Calculate overall trend\n",
        "                    score_diff = last_score - first_score\n",
        "\n",
        "                    if abs(score_diff) >= 1.0:\n",
        "                        direction = \"increased\" if score_diff > 0 else \"decreased\"\n",
        "                        f.write(f\"- {asp} {direction} by {abs(score_diff):.1f} points from {first_year} to {last_year}\\n\")\n",
        "\n",
        "                        # Get the justifications for the latest year\n",
        "                        latest_just = next((row['Justification'] for row in justs_rows\n",
        "                                           if row['Company'] == company and row['Year'] == last_year and row['Aspect'] == asp), None)\n",
        "\n",
        "                        if latest_just:\n",
        "                            f.write(f\"  - Latest assessment: {latest_just}\\n\")\n",
        "\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(f\"Summary report saved to {output_path}\")\n",
        "\n",
        "# — Main loop\n",
        "def main():\n",
        "    scores_rows, justs_rows = [], []\n",
        "\n",
        "    # Check if previous output exists and load it\n",
        "    existing_scores = {}\n",
        "    if os.path.exists(OUTPUT_SCORES):\n",
        "        existing_df = pd.read_csv(OUTPUT_SCORES)\n",
        "        for _, row in existing_df.iterrows():\n",
        "            company = row['Company']\n",
        "            year = row['Year']\n",
        "            if company not in existing_scores:\n",
        "                existing_scores[company] = {}\n",
        "            existing_scores[company][year] = {asp: row[asp] for asp in INTANGIBLE_ASPECTS}\n",
        "\n",
        "        # Also load existing rows for continuity\n",
        "        scores_rows = existing_df.to_dict('records')\n",
        "\n",
        "    existing_justs = {}\n",
        "    if os.path.exists(OUTPUT_JUSTS):\n",
        "        justs_df = pd.read_csv(OUTPUT_JUSTS)\n",
        "        for _, row in justs_df.iterrows():\n",
        "            company = row['Company']\n",
        "            year = row['Year']\n",
        "            aspect = row['Aspect']\n",
        "            if company not in existing_justs:\n",
        "                existing_justs[company] = {}\n",
        "            if year not in existing_justs[company]:\n",
        "                existing_justs[company][year] = {}\n",
        "            existing_justs[company][year][aspect] = row['Justification']\n",
        "\n",
        "        # Also load existing rows for continuity\n",
        "        justs_rows = justs_df.to_dict('records')\n",
        "\n",
        "    for company in sorted(os.listdir(BASE_DIR)):\n",
        "        comp_dir = os.path.join(BASE_DIR, company)\n",
        "        if not os.path.isdir(comp_dir):\n",
        "            continue\n",
        "\n",
        "        # Initialize with slightly varied baselines for each company\n",
        "        prev_scores = initialize_baseline_scores()\n",
        "        prev_summary = {asp: 'Initial baseline assessment.' for asp in INTANGIBLE_ASPECTS}\n",
        "\n",
        "        # Sort files by year for proper sequential analysis\n",
        "        files = sorted(os.listdir(comp_dir))\n",
        "        years_processed = []\n",
        "\n",
        "        # Find the first year's file for this company\n",
        "        first_year = None\n",
        "        for fname in files:\n",
        "            if fname.endswith('.txt'):\n",
        "                year = os.path.splitext(fname)[0].split('_')[-1]\n",
        "                first_year = year\n",
        "                break\n",
        "\n",
        "        for fname in files:\n",
        "            if not fname.endswith('.txt'):\n",
        "                continue\n",
        "\n",
        "            year = os.path.splitext(fname)[0].split('_')[-1]\n",
        "            print(f\"→ {company} {year}\")\n",
        "\n",
        "            # If this is the first year and we don't have prior data, use baseline\n",
        "            if year == first_year and not (company in existing_scores and year in existing_scores[company]):\n",
        "                # Use baseline scores - no prior year to compare with\n",
        "                pass\n",
        "            # Otherwise, load previous year's data if available\n",
        "            elif company in existing_scores:\n",
        "                # Find the previous year's data\n",
        "                prev_year = None\n",
        "                for y in sorted(existing_scores[company].keys()):\n",
        "                    if y < year:  # Find the most recent prior year\n",
        "                        prev_year = y\n",
        "\n",
        "                if prev_year:\n",
        "                    prev_scores = existing_scores[company][prev_year]\n",
        "                    if company in existing_justs and prev_year in existing_justs[company]:\n",
        "                        prev_summary = existing_justs[company][prev_year]\n",
        "\n",
        "            # Skip if already processed this specific year\n",
        "            if company in existing_scores and year in existing_scores[company]:\n",
        "                print(f\"  Already processed {company} {year}, skipping...\")\n",
        "                years_processed.append(year)\n",
        "                continue\n",
        "\n",
        "            # Load and chunk the filing\n",
        "            text = open(os.path.join(comp_dir, fname), encoding='utf-8').read()\n",
        "            chunks = chunk_text(text)\n",
        "\n",
        "            # Per-aspect storage\n",
        "            year_deltas = {asp: [] for asp in INTANGIBLE_ASPECTS}\n",
        "            year_justs = {asp: [] for asp in INTANGIBLE_ASPECTS}\n",
        "\n",
        "            # Process each chunk\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                print(f\"  Processing chunk {chunk_idx+1}/{len(chunks)}\")\n",
        "\n",
        "                for asp in INTANGIBLE_ASPECTS:\n",
        "                    # 1) Extract relevant text\n",
        "                    relevant_text = extract_aspect_text(chunk, asp)\n",
        "\n",
        "                    # 2) Run VADER sentiment\n",
        "                    vader_score = vader_delta(relevant_text)\n",
        "\n",
        "                    # 3) Run GPT for that aspect\n",
        "                    gpt_delta, just = analyze_aspect_chunk(\n",
        "                        relevant_text, asp, prev_scores[asp], prev_summary.get(asp, 'No prior assessment available.')\n",
        "                    )\n",
        "\n",
        "                    if just == \"Analysis failed after multiple attempts.\":\n",
        "                        continue\n",
        "\n",
        "                    # 4) Calculate combined delta (weighted toward GPT)\n",
        "                    combined_delta = (gpt_delta * 0.7) + (vader_score * 0.3)\n",
        "\n",
        "                    # Store results\n",
        "                    year_deltas[asp].append(combined_delta)\n",
        "                    year_justs[asp].append(just)\n",
        "\n",
        "                # Pause to ease rate limiting\n",
        "                time.sleep(1.5)\n",
        "\n",
        "            # Calculate new scores and choose justifications\n",
        "            new_scores, new_summary = {}, {}\n",
        "\n",
        "            for asp in INTANGIBLE_ASPECTS:\n",
        "                deltas = year_deltas[asp]\n",
        "                justs = year_justs[asp]\n",
        "\n",
        "                if deltas:\n",
        "                    # Take average of deltas\n",
        "                    avg_delta = sum(deltas) / len(deltas)\n",
        "\n",
        "                    # Calculate new score with improved logic\n",
        "                    new_score = calculate_new_score(prev_scores[asp], avg_delta)\n",
        "\n",
        "                    # Choose justification closest to average delta\n",
        "                    idx = min(range(len(deltas)), key=lambda i: abs(deltas[i] - avg_delta))\n",
        "                    justification = justs[idx]\n",
        "                else:\n",
        "                    # If no data, maintain previous score but note the lack of information\n",
        "                    new_score = prev_scores[asp]\n",
        "                    justification = \"Insufficient data for meaningful analysis.\"\n",
        "\n",
        "                new_scores[asp] = new_score\n",
        "                new_summary[asp] = justification\n",
        "\n",
        "            # Append results\n",
        "            scores_row = {'Company': company, 'Year': year}\n",
        "            scores_row.update(new_scores)\n",
        "            scores_rows.append(scores_row)\n",
        "\n",
        "            for asp in INTANGIBLE_ASPECTS:\n",
        "                justs_rows.append({\n",
        "                    'Company': company,\n",
        "                    'Year': year,\n",
        "                    'Aspect': asp,\n",
        "                    'Justification': new_summary[asp]\n",
        "                })\n",
        "\n",
        "            # Save after each company-year\n",
        "            pd.DataFrame(scores_rows).to_csv(OUTPUT_SCORES, index=False)\n",
        "            pd.DataFrame(justs_rows).to_csv(OUTPUT_JUSTS, index=False)\n",
        "\n",
        "            # Create a summary report\n",
        "            summary_path = '/content/drive/MyDrive/intangible_summary_report.md'\n",
        "            save_summary_report(scores_rows, justs_rows, summary_path)\n",
        "\n",
        "            print(f\"Saved through {company} {year}\")\n",
        "\n",
        "            # Update baselines for next year\n",
        "            prev_scores = new_scores\n",
        "            prev_summary = new_summary\n",
        "            years_processed.append(year)\n",
        "\n",
        "    print(\"✅ All done!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "woxycKQ0oNMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcMtqe6OsZ6OGTjF798nWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}