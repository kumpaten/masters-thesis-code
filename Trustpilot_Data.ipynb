{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/xWIh0PCIuG0nnqp/KjR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumpaten/masters-thesis-code/blob/main/Trustpilot_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "id": "hGEvBrwnPUda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5F6V2CfR-pxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import time\n",
        "import random\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from google.colab import files\n",
        "\n",
        "def scrape_reviews_selenium(url):\n",
        "    # 1. Set up headless Chrome options.\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    # 2. Load the URL.\n",
        "    driver.get(url)\n",
        "\n",
        "    # 3. Wait for dynamic content to load (adjust if needed).\n",
        "    time.sleep(3)\n",
        "\n",
        "    # 4. Find all <article> elements under the main content container\n",
        "    #    Each <article> typically represents a single review\n",
        "    article_elements = driver.find_elements(\n",
        "        By.CSS_SELECTOR,\n",
        "        \"#__next > div > div > main > div > div.styles_mainContent__d9oos > section article\"\n",
        "    )\n",
        "\n",
        "    reviews = []\n",
        "    for article in article_elements:\n",
        "        # 4a. Extract the date from <time datetime=\"...\">\n",
        "        try:\n",
        "            time_elem = article.find_element(By.CSS_SELECTOR, \"time[datetime]\")\n",
        "            review_date = time_elem.get_attribute(\"datetime\")\n",
        "        except:\n",
        "            review_date = None\n",
        "\n",
        "        # 4b. Extract the rating from <div data-service-review-rating=\"...\">\n",
        "        try:\n",
        "            rating_div = article.find_element(By.CSS_SELECTOR, \"div[data-service-review-rating]\")\n",
        "            rating = rating_div.get_attribute(\"data-service-review-rating\")\n",
        "        except:\n",
        "            rating = None\n",
        "\n",
        "        if rating and review_date:\n",
        "            reviews.append({\n",
        "                \"rating\": rating,\n",
        "                \"date\": review_date\n",
        "            })\n",
        "\n",
        "    driver.quit()\n",
        "    return reviews\n",
        "\n",
        "def save_reviews_to_csv(reviews, filename=\"reviews.csv\"):\n",
        "    fieldnames = [\"company\", \"rating\", \"date\"]\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for review in reviews:\n",
        "            writer.writerow(review)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example list of tuples: (company_name, total_pages), ADJUST \"total number of pages\" accordingly to the current state of https://www.trustpilot.com/review/www.{company}.com?languages=all\n",
        "    companies = [(\"google\", \"total number of pages\"), (\"facebook\", \"total number of pages\"), (\"microsoft\", \"total number of pages\"), (\"apple\", \"total number of pages\")]\n",
        "\n",
        "    all_reviews = []\n",
        "    empty_pages = []  # Store (company, page_num) for pages that return zero reviews\n",
        "\n",
        "    # set the \"endpages\" values (2nd parameter of the range() function) at the page count that marks the cut of the time span you are working with (in my case it was the year 2022)\n",
        "    endpages = [\"x\", \"y\", \"z\", \"m\"]\n",
        "    for endpage in endpages:\n",
        "      for company, total_pages in companies:\n",
        "          for num in range(total_pages, endpage, -1):\n",
        "              url = f\"https://www.trustpilot.com/review/{company}.com?languages=all&page={num}\"\n",
        "              print(f\"Scraping URL: {url}\")\n",
        "              review_data = scrape_reviews_selenium(url)\n",
        "\n",
        "              # Tag each review with the company\n",
        "              if review_data:\n",
        "                  for review in review_data:\n",
        "                      review[\"company\"] = company\n",
        "                      all_reviews.append(review)\n",
        "                  print(\"Latest review scraped:\", all_reviews[-1])\n",
        "              else:\n",
        "                  # If no reviews found, store page for re-checking later\n",
        "                  empty_pages.append((company, num))\n",
        "\n",
        "              # Sleep randomly between 1 to 3 seconds\n",
        "              time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    # After the main loop, try re-checking empty pages in a while loop\n",
        "      retry_count = 0\n",
        "\n",
        "      while empty_pages:\n",
        "          retry_count += 1\n",
        "          print(f\"\\nRe-checking {len(empty_pages)} empty pages (attempt {retry_count}) \"\n",
        "                f\"after a 60-second pause...\\n\")\n",
        "          time.sleep(60)  # Wait a bit before retrying\n",
        "\n",
        "          # We'll build a new list for pages that remain empty after this retry\n",
        "          new_empty_pages = []\n",
        "\n",
        "          for (company, page_num) in empty_pages:\n",
        "              url = f\"https://www.trustpilot.com/review/{company}.com?languages=all&page={page_num}\"\n",
        "              print(f\"Re-checking URL: {url}\")\n",
        "              review_data = scrape_reviews_selenium(url)\n",
        "\n",
        "              if review_data:\n",
        "                  # We found reviews this time, so add them to all_reviews\n",
        "                  for review in review_data:\n",
        "                      review[\"company\"] = company\n",
        "                      all_reviews.append(review)\n",
        "                  print(\"Latest review scraped (retry):\", all_reviews[-1])\n",
        "              else:\n",
        "                  # Still empty, keep it for the next retry\n",
        "                  new_empty_pages.append((company, page_num))\n",
        "\n",
        "          # If we made no progress, break to avoid infinite loops\n",
        "          if len(new_empty_pages) == len(empty_pages):\n",
        "            print(\"\\nNo progress made on retry, stopping.\\n\")\n",
        "            break\n",
        "\n",
        "          empty_pages = new_empty_pages\n",
        "\n",
        "          if empty_pages:\n",
        "              print(f\"\\nEven after retries, {len(empty_pages)} pages had zero reviews:\\n{empty_pages}\\n\")\n",
        "          else:\n",
        "              print(\"\\nAll previously empty pages yielded reviews on retry!\\n\")\n",
        "\n",
        "\n",
        "    # Finally, save to CSV\n",
        "    if all_reviews:\n",
        "        save_reviews_to_csv(all_reviews)\n",
        "        print(f\"Scraped {len(all_reviews)} reviews. Data saved to 'reviews.csv'.\")\n",
        "        files.download(\"reviews.csv\")\n",
        "    else:\n",
        "        print(\"No reviews found at all.\")\n",
        "\n",
        "    # Print a sample of the scraped reviews\n",
        "    print(\"Sample reviews:\", all_reviews[:10])\n"
      ],
      "metadata": {
        "id": "ngPlqMGWFcFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the CSV file that you downloaded before and potentially renamed\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Trustpilot_reviews.csv\")\n",
        "\n",
        "# 2. Convert 'date' to datetime and extract 'year'\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "df[\"year\"] = df[\"date\"].dt.year\n",
        "\n",
        "# 3. Group by (company, year) to get average rating and review count\n",
        "grouped = df.groupby([\"company\", \"year\"]).agg(\n",
        "    avg_rating=(\"rating\", \"mean\"),\n",
        "    review_count=(\"rating\", \"size\")\n",
        ").reset_index()\n",
        "\n",
        "# 4. Compute the company-specific average rating across all years (the prior, m_c)\n",
        "grouped[\"company_avg\"] = grouped.groupby(\"company\")[\"avg_rating\"].transform(\"mean\")\n",
        "\n",
        "# 5. Calculate company-specific median review count (firm_C)\n",
        "def median_nonzero(x):\n",
        "    nonzero = x[x > 0]\n",
        "    return int(np.median(nonzero)) if len(nonzero) > 0 else 1\n",
        "\n",
        "grouped[\"firm_C\"] = grouped.groupby(\"company\")[\"review_count\"].transform(median_nonzero)\n",
        "\n",
        "# 6. Apply the Bayesian smoothing formula with firm-specific C\n",
        "grouped[\"smoothed_rating\"] = (\n",
        "    (grouped[\"review_count\"] * grouped[\"avg_rating\"] + grouped[\"firm_C\"] * grouped[\"company_avg\"]) /\n",
        "    (grouped[\"review_count\"] + grouped[\"firm_C\"])\n",
        ")\n",
        "\n",
        "# 7. Sort by (company, year) for clarity\n",
        "grouped.sort_values([\"company\", \"year\"], inplace=True)\n",
        "\n",
        "# 8. (Optional) Save to a new CSV\n",
        "# grouped.to_csv(\"Trustpilot_reviews_yearly_smoothed.csv\", index=False)\n",
        "\n",
        "print(\"Done! The output CSV has columns: company, year, avg_rating, review_count, company_avg, firm_C, and smoothed_rating.\")\n",
        "print(grouped.head())"
      ],
      "metadata": {
        "id": "t1E9mPfOKTIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 2. Filter rows where year is between 2008 and 2021 (inclusive)\n",
        "grouped_filtered = grouped[\n",
        "    (grouped[\"year\"] >= 2008) & (grouped[\"year\"] <= 2021)\n",
        "].copy()  # explicitly create a copy\n",
        "\n",
        "# 3. (Optional) Sort again by (company, year) if needed\n",
        "grouped_filtered.sort_values([\"company\", \"year\"], inplace=True, ignore_index=True)\n",
        "\n",
        "# 4. Save the filtered DataFrame\n",
        "output_file_path = \"Trustpilot_yearly_ratings.csv\"\n",
        "grouped_filtered.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Done! 'grouped_filtered' now only has rows from 2008 to 2021.\")\n",
        "print(grouped_filtered.head())\n",
        "\n",
        "# 5. Save to google drive\n",
        "output_path = '/content/drive/My Drive/Preprocessed_data/Trustpilot_reviews_updated.csv'\n",
        "grouped_filtered.to_csv(output_path, index=False)\n",
        "print(\"File saved to google drive\")\n",
        "\n"
      ],
      "metadata": {
        "id": "E1cCAp1FUnJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}