{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNthZzx/ysox85liWkK/YDq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumpaten/masters-thesis-code/blob/main/DataRandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFfkT-aTUzEL",
        "outputId": "7af7f12b-f1b7-4b0f-ebdb-9fd454da9e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Monolith\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # For potentially nicer default styles if needed, though we customize\n",
        "\n",
        "# --- LaTeX Style Plotting Setup ---\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "# plt.rcParams['font.serif'] = ['DejaVu Serif', 'Bitstream Vera Serif', 'Liberation Serif', 'Times New Roman'] # Choose available\n",
        "plt.rcParams['axes.labelsize'] = 10\n",
        "plt.rcParams['xtick.labelsize'] = 8\n",
        "plt.rcParams['ytick.labelsize'] = 8\n",
        "plt.rcParams['legend.fontsize'] = 8\n",
        "plt.rcParams['figure.titlesize'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 10\n",
        "plt.rcParams['figure.figsize'] = (8, 6) # Adjust for feature importance plots\n",
        "plt.rcParams['lines.linewidth'] = 1.5\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['savefig.format'] = 'pdf'\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "try:\n",
        "    file_path = '/content/drive/MyDrive/Data_to_analyze/Final_Dataset_transformed.csv'\n",
        "    df_original = pd.read_csv(file_path, sep=';')\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: 'Final_Dataset_transformed.csv' not found. Please upload it or check the path.\")\n",
        "    # Dummy data for testing\n",
        "    data_size = 140\n",
        "    firms = [f'Comp{i%10}' for i in range(data_size)]\n",
        "    years = [2008 + i//10 for i in range(data_size)][:data_size]\n",
        "    df_original = pd.DataFrame({\n",
        "        'company': firms[:len(years)], 'year': years,\n",
        "        'ln(Tobin’s Q)': np.random.rand(len(years)) * 2 + 0.5,\n",
        "        'ln(PE)': np.random.rand(len(years)) * 30 + 5,\n",
        "        'ln(TSR)': np.random.rand(len(years)) * 0.5 - 0.1,\n",
        "        'brand_value': np.random.rand(len(years)) * 1000,\n",
        "        'patent_claims': np.random.randint(0, 500, size=len(years)),\n",
        "        'employee_rating': np.random.rand(len(years)) * 2 + 3,\n",
        "        'RD_Intensity': np.random.rand(len(years)) * 0.15,\n",
        "        'SGA_Intensity': np.random.rand(len(years)) * 0.25,\n",
        "        'YJ_Sentiment_PCR': np.random.randn(len(years)),\n",
        "        'is_imputed': np.random.choice([0, 1], size=len(years), p=[0.8, 0.2]),\n",
        "        'ln_total_assets_lag1': np.random.rand(len(years)) * 5 + 10,\n",
        "        'ROA_lag1': np.random.rand(len(years)) * 0.2,\n",
        "        'ln_financial_leverage_lag1': np.random.rand(len(years)) * 1 + 0.5,\n",
        "        'delta_ln_S5INFT_lag1': np.random.randn(len(years)) * 0.1,\n",
        "        'delta_ln_GDPWorld_lag1': np.random.randn(len(years)) * 0.02\n",
        "    })\n",
        "    print(\"Using DUMMY DATA for testing Random Forest.\")\n",
        "\n",
        "df_original['year'] = df_original['year'].astype(int)\n",
        "df_original.set_index(['company', 'year'], inplace=True)\n",
        "df = df_original.copy()\n",
        "\n",
        "# --- 2. Define Variable Groups (ensure these match your DataFrame column names) ---\n",
        "dependent_vars_list = ['ln(Tobin’s Q)', 'ln(PE)', 'ln(TSR)']\n",
        "contemporaneous_intangibles = [\n",
        "    'brand_value', 'patent_claims', 'employee_smoothed_rating',\n",
        "    'R&D_Intensity', 'SG&A_Intensity', 'YJ(Sentiment_PCR)'\n",
        "]\n",
        "# Using only 'ln_total_assets_lag1' as the size control\n",
        "firm_controls = [\n",
        "    'ln_totalAssets-1', 'ROA-1', 'ln(Financial Leverage-1)'\n",
        "]\n",
        "macro_controls = [\n",
        "    'delta_ln_S5INFT-1', 'delta_ln_GDPWorld-1'\n",
        "]\n",
        "dummy_controls_contemp = [\"is_imputed\"]\n",
        "\n",
        "# Create lagged intangibles and lagged dummy\n",
        "lagged_intangibles = []\n",
        "for col in contemporaneous_intangibles:\n",
        "    lagged_col_name = f'{col}_lag1'\n",
        "    df[lagged_col_name] = df.groupby(level='company')[col].shift(1)\n",
        "    lagged_intangibles.append(lagged_col_name)\n",
        "df['is_imputed_lag1'] = df.groupby(level='company')['is_imputed'].shift(1)\n",
        "dummy_controls_lagged = [\"is_imputed_lag1\"]\n",
        "\n",
        "# --- 3. Data Preparation Function for RF (Same as Lasso) ---\n",
        "def prepare_data_for_ml(dataf, dep_var_name, predictor_vars_list):\n",
        "    \"\"\"Demeans and standardizes data for RF/Lasso, handling NaNs.\"\"\"\n",
        "    cols_to_use = [dep_var_name] + predictor_vars_list\n",
        "    missing_cols_in_df = [col for col in cols_to_use if col not in dataf.columns]\n",
        "    if missing_cols_in_df:\n",
        "        print(f\"ERROR in prepare_data_for_ml: Columns {missing_cols_in_df} not found.\")\n",
        "        return None, None, None\n",
        "\n",
        "    panel_data = dataf[cols_to_use].copy()\n",
        "    demeaned_data = panel_data.groupby(level='company').transform(lambda x: x - x.mean())\n",
        "\n",
        "    y_demeaned = demeaned_data[dep_var_name]\n",
        "    X_demeaned = demeaned_data[predictor_vars_list]\n",
        "\n",
        "    combined_for_dropna = pd.concat([y_demeaned, X_demeaned], axis=1).dropna()\n",
        "    if combined_for_dropna.empty or len(combined_for_dropna) < 10: # Min obs check\n",
        "        print(f\"Warning: Dataframe empty/too small after demeaning and dropna for DV: {dep_var_name}.\")\n",
        "        return None, None, None\n",
        "\n",
        "    y_final = combined_for_dropna[dep_var_name]\n",
        "    X_final_demeaned = combined_for_dropna[predictor_vars_list]\n",
        "\n",
        "    if not X_final_demeaned.empty:\n",
        "        scaler = StandardScaler()\n",
        "        X_final_standardized_array = scaler.fit_transform(X_final_demeaned)\n",
        "        X_final_standardized_df = pd.DataFrame(X_final_standardized_array, columns=X_final_demeaned.columns, index=X_final_demeaned.index)\n",
        "        return y_final, X_final_standardized_df, X_final_demeaned.columns # Return column names\n",
        "    else:\n",
        "        print(f\"Warning: X_final_demeaned is empty for DV: {dep_var_name}. Cannot standardize.\")\n",
        "        return None, None, None\n",
        "\n",
        "# --- 4. Random Forest Analysis Function ---\n",
        "def run_rf_feature_importance(y, X, X_cols, model_name_display, dep_var_name, scenario_label, test_size=0.25, random_state=42):\n",
        "    \"\"\"Runs Random Forest, evaluates, and extracts feature importances.\"\"\"\n",
        "    if y is None or X is None or X.empty or y.empty or len(y) < 20 : # Min obs for split\n",
        "        print(f\"Skipping RF for {model_name_display} (DV: {dep_var_name}) due to insufficient data after prep (Obs: {len(y) if y is not None else 0}).\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(f\"\\n--- Fitting Random Forest for: {model_name_display} (DV: {dep_var_name}) ---\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    if len(X_train) < 10 or len(X_test) < 5: # Ensure splits are meaningful\n",
        "        print(f\"  Skipping {model_name_display}: Train/Test split resulted in too few samples (Train: {len(X_train)}, Test: {len(X_test)}).\")\n",
        "        return None, None, None\n",
        "\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=200,       # Reasonable number of trees\n",
        "        max_depth=None,           # Control tree depth (adjust based on p)\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        oob_score=True,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1               # Use all available cores\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        rf_model.fit(X_train, y_train)\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR fitting RandomForestRegressor for {model_name_display}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Basic Performance Metrics\n",
        "    oob_r2 = rf_model.oob_score_\n",
        "    test_r2 = r2_score(y_test, rf_model.predict(X_test))\n",
        "    print(f\"  OOB R-squared: {oob_r2:.4f}\")\n",
        "    print(f\"  Test Set R-squared: {test_r2:.4f}\")\n",
        "\n",
        "    # Feature Importance Assessment\n",
        "    # 1. Permutation Importance (on test set)\n",
        "    print(\"  Calculating Permutation Importance...\")\n",
        "    perm_importance = permutation_importance(\n",
        "        rf_model, X_test, y_test, n_repeats=10, random_state=random_state, n_jobs=-1\n",
        "    )\n",
        "    perm_importance_df = pd.DataFrame({\n",
        "        'feature': X_cols,\n",
        "        'importance_mean': perm_importance.importances_mean,\n",
        "        'importance_std': perm_importance.importances_std\n",
        "    }).sort_values(by='importance_mean', ascending=False)\n",
        "    print(\"\\n  Permutation Importances (Test Set):\")\n",
        "    print(perm_importance_df)\n",
        "\n",
        "    # Plot Permutation Importance\n",
        "    plt.figure(figsize=(8, 0.25 * len(X_cols) + 2)) # Dynamic height\n",
        "    sorted_idx = perm_importance.importances_mean.argsort()\n",
        "    plt.barh(X_cols[sorted_idx], perm_importance.importances_mean[sorted_idx],\n",
        "             xerr=perm_importance.importances_std[sorted_idx], align='center', color='skyblue', ecolor='gray')\n",
        "    plt.xlabel(\"Permutation Importance (Mean Decrease in R-squared)\")\n",
        "    plt.ylabel(\"Predictor\")\n",
        "    clean_dep_var = dep_var_name.replace(\"(\", \"\").replace(\")\", \"\").replace(\"’\", \"\").replace(\" \", \"_\")\n",
        "    clean_model_name = model_name_display.replace(\" \",\"_\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"+\",\"plus\")\n",
        "    plt.title(f'RF Permutation Importance: {clean_dep_var}\\n{scenario_label} - {clean_model_name}', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plot_filename_perm = f\"rf_perm_importance_{clean_dep_var}_{scenario_label}_{clean_model_name}.pdf\"\n",
        "    plt.savefig(plot_filename_perm)\n",
        "    plt.show()\n",
        "    print(f\"  Saved Permutation Importance plot to: {plot_filename_perm}\")\n",
        "\n",
        "    # 2. Mean Decrease in Impurity (MDI - Gini importance for regression)\n",
        "    mdi_importance = rf_model.feature_importances_\n",
        "    mdi_importance_df = pd.DataFrame({\n",
        "        'feature': X_cols,\n",
        "        'mdi_importance': mdi_importance\n",
        "    }).sort_values(by='mdi_importance', ascending=False)\n",
        "    print(\"\\n  Mean Decrease in Impurity (MDI) Importances:\")\n",
        "    print(mdi_importance_df)\n",
        "\n",
        "    # Plot MDI Importance\n",
        "    plt.figure(figsize=(8, 0.25 * len(X_cols) + 2)) # Dynamic height\n",
        "    sorted_mdi_idx = mdi_importance_df['mdi_importance'].argsort()\n",
        "    plt.barh(mdi_importance_df['feature'].iloc[sorted_mdi_idx], mdi_importance_df['mdi_importance'].iloc[sorted_mdi_idx],\n",
        "             align='center', color='lightcoral')\n",
        "    plt.xlabel(\"Mean Decrease in Impurity (MDI)\")\n",
        "    plt.ylabel(\"Predictor\")\n",
        "    plt.title(f'RF MDI Feature Importance: {clean_dep_var}\\n{scenario_label} - {clean_model_name}', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plot_filename_mdi = f\"rf_mdi_importance_{clean_dep_var}_{scenario_label}_{clean_model_name}.pdf\"\n",
        "    plt.savefig(plot_filename_mdi)\n",
        "    plt.show()\n",
        "    print(f\"  Saved MDI Importance plot to: {plot_filename_mdi}\")\n",
        "\n",
        "    return rf_model, perm_importance_df, mdi_importance_df\n",
        "\n",
        "# --- 5. Define Model Specifications for RF (Full Models Only) ---\n",
        "rf_scenarios = {}\n",
        "\n",
        "# Scenario RF1c (Full Contemporaneous Model)\n",
        "s1_rf_model_c_preds = contemporaneous_intangibles + dummy_controls_contemp + firm_controls + macro_controls\n",
        "rf_scenarios[\"Scenario_RF1c_Contemp_Full\"] = {\n",
        "    \"C_Full\": {\"label\": \"RF Model C (Contemporaneous Full)\", \"predictors\": s1_rf_model_c_preds, \"intangible_type\": \"Contemporaneous\"}\n",
        "}\n",
        "\n",
        "# Scenario RF2c (Full Lagged Model)\n",
        "s2_rf_model_c_preds = lagged_intangibles + dummy_controls_lagged + firm_controls + macro_controls\n",
        "rf_scenarios[\"Scenario_RF2c_Lagged_Full\"] = {\n",
        "    \"C_Full\": {\"label\": \"RF Model C (Lagged Full)\", \"predictors\": s2_rf_model_c_preds, \"intangible_type\": \"Lagged\"}\n",
        "}\n",
        "\n",
        "# --- 6. Execute Random Forest Analysis ---\n",
        "all_rf_results = {} # To store models and importance dataframes\n",
        "\n",
        "for dv in dependent_vars_list:\n",
        "    all_rf_results[dv] = {}\n",
        "    print(f\"\\n\\n{'='*80}\")\n",
        "    print(f\" RANDOM FOREST ANALYSIS FOR DEPENDENT VARIABLE: {dv} \".center(80, \"=\"))\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for scenario_key_rf, models_rf in rf_scenarios.items():\n",
        "        all_rf_results[dv][scenario_key_rf] = {}\n",
        "        print(f\"\\n--- {scenario_key_rf.replace('_', ' ')} ---\")\n",
        "\n",
        "        for model_key_rf, spec_rf in models_rf.items(): # Should only be 'C_Full'\n",
        "            model_label_rf = spec_rf[\"label\"]\n",
        "            predictors_rf = spec_rf[\"predictors\"]\n",
        "\n",
        "            y_prepared_rf, X_prepared_rf, X_col_names_rf = prepare_data_for_ml(df, dv, predictors_rf)\n",
        "\n",
        "            if y_prepared_rf is not None and X_prepared_rf is not None and not X_prepared_rf.empty:\n",
        "                fitted_rf, perm_df, mdi_df = run_rf_feature_importance(\n",
        "                    y_prepared_rf, X_prepared_rf, X_col_names_rf, model_label_rf, dv, scenario_key_rf\n",
        "                )\n",
        "                all_rf_results[dv][scenario_key_rf][model_label_rf] = {\n",
        "                    \"model\": fitted_rf,\n",
        "                    \"permutation_importance\": perm_df,\n",
        "                    \"mdi_importance\": mdi_df\n",
        "                }\n",
        "            else:\n",
        "                print(f\"Skipping RF for {model_label_rf} (DV: {dv}) due to data preparation issues.\")\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*30 + \" END OF RANDOM FOREST ANALYSIS \" + \"=\"*30)"
      ],
      "metadata": {
        "id": "ftVWvjOHeASI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}