{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUt9zEkGXJB021nYsUdH3P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumpaten/masters-thesis-code/blob/main/DataOLS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Install required package\n",
        "!pip install linearmodels\n",
        "\n",
        "# 1) Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1AykEHonmR3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OLS Monolith\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.api import het_breuschpagan\n",
        "from statsmodels.stats.stattools import durbin_watson, jarque_bera\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from linearmodels.panel import PanelOLS\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# --- LaTeX Style Plotting Setup ---\n",
        "# (This should be adjusted based on your exact thesis LaTeX font and style if possible)\n",
        "# For now, using common settings that look professional.\n",
        "plt.style.use('seaborn-v0_8-whitegrid') # A clean style\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = ['DejaVu Sans'] # Or a list of preferred fallbacks\n",
        "# Remove or comment out: plt.rcParams['font.serif'] = ['Times New Roman']\n",
        "plt.rcParams['axes.labelsize'] = 10\n",
        "plt.rcParams['xtick.labelsize'] = 8\n",
        "plt.rcParams['ytick.labelsize'] = 8\n",
        "plt.rcParams['legend.fontsize'] = 8\n",
        "plt.rcParams['figure.titlesize'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 10\n",
        "plt.rcParams['figure.figsize'] = (6, 4) # Adjust as needed for thesis layout (e.g., half-width)\n",
        "plt.rcParams['lines.linewidth'] = 1.5\n",
        "# For saving figures that LaTeX can use well:\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['savefig.format'] = 'pdf' # or 'png' or 'eps'\n",
        "\n",
        "# 1) Load preprocessed dataset\n",
        "# Make sure to upload your CSV to Colab or adjust the path\n",
        "try:\n",
        "    file_path = '/content/drive/MyDrive/Data_to_analyze/Final_Dataset_transformed.csv'\n",
        "    df_original = pd.read_csv(file_path, sep=';')\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: 'Final_Dataset_transformed.csv' not found. Please upload it or check the path.\")\n",
        "    # Fallback for testing if file is missing:\n",
        "    # df_original = pd.DataFrame(np.random.rand(100, 15), columns=['company', 'year', 'ln(Tobin’s Q)', 'ln(PE)', 'ln(TSR)', 'brand_value', 'patent_claims', 'employee_smoothed_rating', 'R&D_Intensity', 'SG&A_Intensity', 'YJ(Sentiment_PCR)', 'ln_totalAssets-1', 'ln_totalSales-1', 'ROA-1', 'ln(Financial Leverage-1)', 'delta_ln_S5INFT-1', 'delta_ln_GDPWorld-1', 'is_imputed'])\n",
        "    # df_original['company'] = [f'Comp{i%10}' for i in range(100)]\n",
        "    # df_original['year'] = [2010 + i//10 for i in range(100)]\n",
        "\n",
        "\n",
        "df_original['year'] = df_original['year'].astype(int)\n",
        "df_original.set_index(['company', 'year'], inplace=True)\n",
        "\n",
        "# 2) Define variable groups (ensure names exactly match CSV columns)\n",
        "dependent_vars_list = ['ln(Tobin’s Q)', 'ln(PE)', 'ln(TSR)']\n",
        "\n",
        "# Ensure these names are EXACTLY as in your CSV after any transformations you've done\n",
        "contemporaneous_intangibles_base = [\n",
        "    'Brand Value (USD m)', 'Patent Claims', 'Employee Rating', # Assuming these are the base names\n",
        "    'R&D Intensity (%)', 'SG&A Intensity (%)', 'YJ(Sentiment_PCR)'\n",
        "]\n",
        "# Renaming to match typical Python variable naming conventions if needed, or ensure CSV matches\n",
        "# For simplicity, I'll assume your CSV uses these or similar Python-friendly names:\n",
        "contemporaneous_intangibles = [\n",
        "    'brand_value', 'patent_claims', 'employee_smoothed_rating', # Changed from 'employee_smoothed_rating' for consistency if it's the main proxy\n",
        "    'R&D_Intensity', 'SG&A_Intensity', 'YJ(Sentiment_PCR)' # Using underscores\n",
        "]\n",
        "# Check which set of names is correct for your 'Final_Dataset_transformed.csv'\n",
        "# Let's assume Python-friendly names are in the CSV for now.\n",
        "# If not, you'll need to rename columns in the DataFrame or use the original names here.\n",
        "# Example: df.rename(columns={'Brand Value (USD m)': 'brand_value', ...}, inplace=True)\n",
        "#'ln_totalSales-1'\n",
        "\n",
        "firm_controls = [\n",
        "    'ln_totalAssets-1', 'ROA-1', 'ln(Financial Leverage-1)' # Assuming Python-friendly names\n",
        "]\n",
        "macro_controls = [\n",
        "    'delta_ln_S5INFT-1', 'delta_ln_GDPWorld-1' # Assuming Python-friendly names\n",
        "]\n",
        "dummy_controls = [\"is_imputed\"] # This should be 0 or 1\n",
        "\n",
        "# --- Data Cleaning: Ensure column names match ---\n",
        "# Create a mapping if your CSV column names are different\n",
        "column_name_mapping = {\n",
        "    'Brand Value (USD m)': 'brand_value',\n",
        "    'Patent Claims': 'patent_claims',\n",
        "    'Employee Rating': 'employee_smoothed_rating', # This was 'employee_smoothed_rating' in your code\n",
        "    'R&D Intensity (%)': 'R&D_Intensity',\n",
        "    'SG&A Intensity (%)': 'SG&A_Intensity',\n",
        "    'YJ(Sentiment_PCR)': 'YJ(Sentiment_PCR)',\n",
        "    'ln(Total Assets-1)': 'ln_totalAssets-1', # Example, adjust if your lag naming is different\n",
        "    #'ln(Total Sales-1)': 'ln_totalSales-1', # ln(Total Sales-1) was dropped due to multicollinearity with ln(Total Assets-1)\n",
        "    'ROA-1': 'ROA-1',\n",
        "    'ln(Financial Leverage-1)': 'ln(Financial Leverage-1)',\n",
        "    'delta_ln_S5INFT-1': 'delta_ln_S5INFT-1',\n",
        "    'delta_ln_GDPWorld-1': 'delta_ln_GDPWorld-1',\n",
        "    'is_imputed': 'is_imputed' # Assuming this is already Python-friendly\n",
        "}\n",
        "# Check if renaming is needed\n",
        "df = df_original.copy()\n",
        "cols_to_rename = {k: v for k, v in column_name_mapping.items() if k in df.columns}\n",
        "if cols_to_rename:\n",
        "    df.rename(columns=cols_to_rename, inplace=True)\n",
        "    print(f\"Renamed columns: {cols_to_rename}\")\n",
        "\n",
        "# Verify all defined variables exist in the DataFrame\n",
        "all_defined_vars = dependent_vars_list + contemporaneous_intangibles + firm_controls + macro_controls + dummy_controls\n",
        "missing_cols = [col for col in all_defined_vars if col not in df.columns and not col.endswith('_lag1')] # _lag1 will be created\n",
        "if missing_cols:\n",
        "    print(f\"ERROR: The following defined variables are missing from the DataFrame: {missing_cols}\")\n",
        "    print(f\"Available columns: {df.columns.tolist()}\")\n",
        "    # exit() # Or handle error appropriately\n",
        "\n",
        "# 3) Create a working copy and generate lagged intangible variables\n",
        "lagged_intangibles = []\n",
        "for col in contemporaneous_intangibles:\n",
        "    lagged_col_name = f'{col}_lag1'\n",
        "    df[lagged_col_name] = df.groupby(level='company')[col].shift(1)\n",
        "    lagged_intangibles.append(lagged_col_name)\n",
        "\n",
        "# Also create lagged dummy control for Scenario 2\n",
        "df['is_imputed_lag1'] = df.groupby(level='company')['is_imputed'].shift(1)\n",
        "lagged_dummy_controls = [\"is_imputed_lag1\"]\n",
        "\n",
        "\n",
        "# 4) Standardize ALL potential predictors that will be used in models\n",
        "# We standardize after creating lagged versions to avoid data leakage from future to past via scaler fitting\n",
        "# Standardize contemporaneous and lagged intangibles, firm controls, and macro controls\n",
        "# Dummy variables are typically NOT standardized.\n",
        "predictors_to_standardize = (\n",
        "    contemporaneous_intangibles +\n",
        "    lagged_intangibles +\n",
        "    firm_controls +\n",
        "    macro_controls\n",
        ")\n",
        "\n",
        "# Ensure no new NaNs were introduced by shift that are not handled by dropna() before scaling\n",
        "df_for_scaler = df[predictors_to_standardize].copy()\n",
        "# Important: Fit the scaler ONLY on data that is not NaN to avoid issues.\n",
        "# Then transform the whole columns, NaNs will remain NaNs.\n",
        "for col in predictors_to_standardize:\n",
        "    if col in df_for_scaler.columns:\n",
        "        col_data_no_na = df_for_scaler[col].dropna()\n",
        "        if not col_data_no_na.empty:\n",
        "            scaler = StandardScaler()\n",
        "            # Fit scaler on non-NaN data\n",
        "            df_for_scaler.loc[col_data_no_na.index, col] = scaler.fit_transform(col_data_no_na.values.reshape(-1, 1))\n",
        "            # Apply transformation to the original dataframe column\n",
        "            df[col] = scaler.transform(df[[col]].values.reshape(-1,1)) # Transform original df column\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' has no non-NaN data for scaler fitting. Will be all NaNs or not scaled.\")\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' not found for standardization.\")\n",
        "\n",
        "\n",
        "# (Keep all imports and previous setup code the same)\n",
        "# ...\n",
        "\n",
        "# 5) MODIFIED Function to run models\n",
        "def run_ols_scenario(\n",
        "    scenario_label,\n",
        "    dep_var_name,\n",
        "    dataf,\n",
        "    intangible_set,\n",
        "    current_dummy_controls,\n",
        "    run_ldv_on_model_c=False, # Renamed for clarity\n",
        "    specific_models_to_run=None # New parameter: e.g., ['A', 'B', 'C'] or ['C']\n",
        "):\n",
        "    fitted_models_dict = {}\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" OLS SCENARIO: {scenario_label} \".center(60, \"=\"))\n",
        "    print(f\" Dependent Variable: {dep_var_name} \".center(60, \"=\"))\n",
        "    intangible_type = 'Contemporaneous' if intangible_set == contemporaneous_intangibles else 'Lagged'\n",
        "    print(f\" Intangible Set: {intangible_type} \".center(60, \"=\"))\n",
        "    if run_ldv_on_model_c and (specific_models_to_run is None or 'C' in specific_models_to_run):\n",
        "        print(\" Model C will include Lagged Dependent Variable (LDV) \".center(60, \"=\"))\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Define base predictors for this scenario\n",
        "    base_model_a_predictors = list(intangible_set) + list(current_dummy_controls)\n",
        "    base_model_b_predictors = base_model_a_predictors + firm_controls\n",
        "    base_model_c_predictors = base_model_b_predictors + macro_controls\n",
        "\n",
        "    # Full set of model specifications\n",
        "    all_models_specs_dict = {\n",
        "        \"A\": {\"label\": \"Model A (Intangibles Only)\", \"predictors\": base_model_a_predictors},\n",
        "        \"B\": {\"label\": \"Model B (+ Firm-Level Controls)\", \"predictors\": base_model_b_predictors},\n",
        "        \"C\": {\"label\": \"Model C (Full Controls)\", \"predictors\": base_model_c_predictors},\n",
        "    }\n",
        "\n",
        "    # Determine which models to actually run\n",
        "    if specific_models_to_run: # If a specific list is provided\n",
        "        models_to_process = {key: all_models_specs_dict[key] for key in specific_models_to_run if key in all_models_specs_dict}\n",
        "    else: # Run all by default\n",
        "        models_to_process = all_models_specs_dict\n",
        "\n",
        "    current_df_scenario = dataf.copy()\n",
        "\n",
        "    dep_var_lag1_name = f'{dep_var_name}_lag1_dv'\n",
        "    # Prepare LDV only if Model C is being run and LDV is requested for it\n",
        "    if 'C' in models_to_process and run_ldv_on_model_c:\n",
        "        current_df_scenario[dep_var_lag1_name] = current_df_scenario.groupby(level='company')[dep_var_name].shift(1)\n",
        "        ldv_data_no_na = current_df_scenario[dep_var_lag1_name].dropna()\n",
        "        if not ldv_data_no_na.empty:\n",
        "            scaler_ldv = StandardScaler()\n",
        "            current_df_scenario.loc[ldv_data_no_na.index, dep_var_lag1_name] = scaler_ldv.fit_transform(ldv_data_no_na.values.reshape(-1, 1))\n",
        "            print(f\"  LDV '{dep_var_lag1_name}' prepared and standardized for Model C.\")\n",
        "        else:\n",
        "            print(f\"Warning: No data to fit scaler for LDV '{dep_var_lag1_name}'. LDV model might fail.\")\n",
        "\n",
        "\n",
        "    for model_key, spec in models_to_process.items():\n",
        "        model_label_base = spec[\"label\"]\n",
        "        actual_predictors = list(spec[\"predictors\"])\n",
        "        display_model_label = model_label_base\n",
        "\n",
        "        if model_key == \"C\" and run_ldv_on_model_c:\n",
        "            if dep_var_lag1_name in current_df_scenario.columns:\n",
        "                actual_predictors.append(dep_var_lag1_name)\n",
        "                display_model_label = f\"{model_label_base} + LDV\"\n",
        "            else:\n",
        "                print(f\"Warning: LDV '{dep_var_lag1_name}' not prepared for {model_label_base}. Running Model C without LDV.\")\n",
        "\n",
        "        print(f\"\\n--- Fitting {display_model_label} ---\")\n",
        "\n",
        "        missing_model_preds = [p for p in actual_predictors if p not in current_df_scenario.columns]\n",
        "        if missing_model_preds:\n",
        "            print(f\"ERROR: Predictors missing for {display_model_label}: {missing_model_preds}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        model_data = current_df_scenario[[dep_var_name] + actual_predictors].dropna()\n",
        "\n",
        "        if model_data.empty or len(model_data) < (len(actual_predictors) + model_data.index.get_level_values('company').nunique() + 5) :\n",
        "            print(f\"No/Insufficient data for {display_model_label} (Obs: {len(model_data)}, Preds: {len(actual_predictors)}). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        y_model = model_data[dep_var_name]\n",
        "        X_model_df = model_data[actual_predictors]\n",
        "\n",
        "        try:\n",
        "            mod = PanelOLS(y_model, sm.add_constant(X_model_df, has_constant='add'), entity_effects=True, drop_absorbed=True)\n",
        "            model_fit = mod.fit(cov_type='clustered', cluster_entity=True)\n",
        "\n",
        "            fitted_models_dict[display_model_label] = model_fit\n",
        "            print(model_fit.summary)\n",
        "\n",
        "            if model_key == \"C\" and run_ldv_on_model_c:\n",
        "                print(\"  Note: Coefficient on Lagged DV may be subject to Nickell bias.\")\n",
        "\n",
        "            # Re-estimate Model A with i.i.d. errors (only if Model A is run and it's not an LDV focused run for Model C)\n",
        "            if model_key == \"A\" and not run_ldv_on_model_c :\n",
        "                 print(f\"\\n--- Re-estimating {model_label_base} with Homoskedastic (i.i.d.) Errors for Comparison ---\")\n",
        "                 mod_iid = PanelOLS(y_model, sm.add_constant(X_model_df, has_constant='add'), entity_effects=True, drop_absorbed=True)\n",
        "                 model_fit_iid = mod_iid.fit()\n",
        "                 print(model_fit_iid.summary)\n",
        "\n",
        "            # --- Diagnostics ---\n",
        "            print(f\"\\n--- Diagnostics for {display_model_label} ({intangible_type} Intangibles) ---\")\n",
        "            residuals = model_fit.resids.copy()\n",
        "            exog_for_bp_test = sm.add_constant(X_model_df.loc[residuals.index], has_constant='add').values\n",
        "\n",
        "            if len(residuals) > exog_for_bp_test.shape[1]:\n",
        "                try:\n",
        "                    bp_test_results = het_breuschpagan(residuals, exog_for_bp_test)\n",
        "                    print(f\"Breusch-Pagan Test: LM Stat={bp_test_results[0]:.3f}, LM p-val={bp_test_results[1]:.3f}, F Stat={bp_test_results[2]:.3f}, F p-val={bp_test_results[3]:.3f}\")\n",
        "                    if bp_test_results[1] < 0.05 or bp_test_results[3] < 0.05: print(\"  Evidence of heteroskedasticity (p < 0.05). Clustered SEs appropriate.\")\n",
        "                    else: print(\"  No strong evidence of heteroskedasticity (p >= 0.05).\")\n",
        "                except Exception as e: print(f\"  Could not run Breusch-Pagan test: {e}\")\n",
        "            else: print(\"  Not enough observations for Breusch-Pagan test after handling NaNs for exog.\")\n",
        "\n",
        "            if len(residuals) > 1:\n",
        "                dw_stat = durbin_watson(residuals)\n",
        "                print(f\"Durbin-Watson Statistic: {dw_stat:.3f}\")\n",
        "                if dw_stat < 1.5: print(\"  Indicates strong positive serial correlation.\")\n",
        "                elif dw_stat > 2.5: print(\"  Indicates strong negative serial correlation.\")\n",
        "                elif 1.5 <= dw_stat < 1.9: print(\"  Indicates some positive serial correlation.\")\n",
        "                elif 2.1 < dw_stat <= 2.5: print(\"  Indicates some negative serial correlation.\")\n",
        "                else: print(\"  DW in inconclusive/no strong serial correlation range (around 2).\")\n",
        "            if len(residuals) > 20:\n",
        "                try:\n",
        "                    n_lags = min(10, len(residuals)//5 -1 )\n",
        "                    if n_lags > 0 :\n",
        "                        lb_results = acorr_ljungbox(residuals, lags=n_lags, return_df=True)\n",
        "                        print(\"Ljung-Box Q Test (up to {} lags):\".format(n_lags))\n",
        "                        print(lb_results)\n",
        "                        if (lb_results['lb_pvalue'] < 0.05).any(): print(\"  Evidence of serial correlation (Ljung-Box). Clustered SEs appropriate.\")\n",
        "                        else: print(\"  No strong evidence of serial correlation (Ljung-Box).\")\n",
        "                    else: print(\"  Not enough observations for meaningful Ljung-Box test lags.\")\n",
        "                except Exception as e: print(f\"  Could not run Ljung-Box test: {e}\")\n",
        "            else: print(\"  Not enough residuals for Ljung-Box test.\")\n",
        "\n",
        "            if len(residuals) > 2:\n",
        "                jb_stat, jb_pvalue, skew, kurtosis = jarque_bera(residuals)\n",
        "                print(f\"Jarque-Bera Test: Stat={jb_stat:.3f}, p-val={jb_pvalue:.3f}, Skew={skew:.3f}, Kurtosis={kurtosis:.3f}\")\n",
        "                if jb_pvalue < 0.05: print(\"  Residuals may not be normally distributed (p < 0.05).\")\n",
        "                else: print(\"  Residuals appear normally distributed (p >= 0.05).\")\n",
        "\n",
        "                plt.figure()\n",
        "                stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "                plot_title = f'Q-Q Plot: {dep_var_name}\\n{display_model_label} ({intangible_type} Intangibles)'\n",
        "                plt.title(plot_title, fontsize=10) # Adjusted title for brevity\n",
        "                plt.xlabel(\"Theoretical Quantiles (Normal Distribution)\")\n",
        "                plt.ylabel(\"Sample Quantiles (Residuals)\")\n",
        "                filename_label = display_model_label.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"+\", \"plus\").replace(\"/\", \"or\")\n",
        "                dv_filename_part = dep_var_name.replace(\"(\", \"\").replace(\")\", \"\").replace(\"’\", \"\").replace(\" \", \"_\")\n",
        "                plot_filename = f\"qq_plot_{dv_filename_part}_{scenario_label}_{filename_label}.pdf\"\n",
        "                plt.savefig(plot_filename, bbox_inches='tight')\n",
        "                plt.show()\n",
        "                print(f\"  Saved Q-Q plot to: {plot_filename}\")\n",
        "            else: print(\"  Not enough residuals for Jarque-Bera test or Q-Q plot.\")\n",
        "\n",
        "            print(\"\\nVariance Inflation Factors (VIF):\")\n",
        "            X_for_vif_calc = X_model_df.dropna()\n",
        "            if X_for_vif_calc.shape[1] > 1 and len(X_for_vif_calc) > X_for_vif_calc.shape[1]:\n",
        "                try:\n",
        "                    vif_data = pd.DataFrame()\n",
        "                    vif_data[\"feature\"] = X_for_vif_calc.columns\n",
        "                    vif_data[\"VIF\"] = [variance_inflation_factor(X_for_vif_calc.values, i) for i in range(X_for_vif_calc.shape[1])]\n",
        "                    print(vif_data.sort_values('VIF', ascending=False))\n",
        "                    if (vif_data[\"VIF\"] > 10).any(): print(\"  Warning: Potential multicollinearity detected (VIF > 10 for some features).\")\n",
        "                    elif (vif_data[\"VIF\"] > 5).any(): print(\"  Note: Some multicollinearity present (VIF > 5 for some features).\")\n",
        "                    else: print(\"  No strong evidence of multicollinearity (all VIFs <= 5).\")\n",
        "                except Exception as e: print(f\"  Could not calculate VIF: {e} (Shape: {X_for_vif_calc.shape})\")\n",
        "            elif X_for_vif_calc.shape[1] <=1 : print(\"  Not enough predictors for VIF (need >1).\")\n",
        "            else: print(f\"  Not enough observations after NaN drop for VIF (Obs: {len(X_for_vif_calc)}, Preds: {X_for_vif_calc.shape[1]}).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not fit or run diagnostics for {display_model_label}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return fitted_models_dict\n",
        "\n",
        "# 6) Execute OLS Analysis as per Methodology\n",
        "\n",
        "# Scenario 1: Contemporaneous Intangibles (Models A, B, C)\n",
        "print(\"\\n\\n\" + \"*\"*80)\n",
        "print(\" EXECUTING OLS SCENARIO 1: CONTEMPORANEOUS INTANGIBLES (Models A, B, C) \".center(80, \"*\"))\n",
        "print(\"*\"*80 + \"\\n\")\n",
        "all_fitted_models_scenario1 = {}\n",
        "for dv in dependent_vars_list:\n",
        "    all_fitted_models_scenario1[dv] = run_ols_scenario(\n",
        "        scenario_label=\"Scenario1_Contemp\",\n",
        "        dep_var_name=dv,\n",
        "        dataf=df.copy(),\n",
        "        intangible_set=contemporaneous_intangibles,\n",
        "        current_dummy_controls=dummy_controls,\n",
        "        run_ldv_on_model_c=False, # Model C here does NOT include LDV\n",
        "        specific_models_to_run=['A', 'B', 'C'] # Explicitly run all three\n",
        "    )\n",
        "\n",
        "# Scenario 1 - Model C with LDV (Robustness Check - ONLY RUN MODEL C)\n",
        "print(\"\\n\\n\" + \"*\"*80)\n",
        "print(\" EXECUTING OLS SCENARIO 1: MODEL C with LDV (ROBUSTNESS) \".center(80, \"*\"))\n",
        "print(\"*\"*80 + \"\\n\")\n",
        "all_fitted_models_scenario1_model_c_ldv = {} # Renamed for clarity\n",
        "for dv in dependent_vars_list:\n",
        "    all_fitted_models_scenario1_model_c_ldv[dv] = run_ols_scenario(\n",
        "        scenario_label=\"Scenario1_Contemp_ModelC_LDV\", # More specific label\n",
        "        dep_var_name=dv,\n",
        "        dataf=df.copy(),\n",
        "        intangible_set=contemporaneous_intangibles,\n",
        "        current_dummy_controls=dummy_controls,\n",
        "        run_ldv_on_model_c=True, # This will augment Model C with LDV\n",
        "        specific_models_to_run=['C'] # KEY CHANGE: Only run Model C\n",
        "    )\n",
        "\n",
        "# Scenario 2: Lagged Intangibles (Models A, B, C)\n",
        "print(\"\\n\\n\" + \"*\"*80)\n",
        "print(\" EXECUTING OLS SCENARIO 2: LAGGED INTANGIBLES (Models A, B, C) \".center(80, \"*\"))\n",
        "print(\"*\"*80 + \"\\n\")\n",
        "all_fitted_models_scenario2 = {}\n",
        "for dv in dependent_vars_list:\n",
        "    all_fitted_models_scenario2[dv] = run_ols_scenario(\n",
        "        scenario_label=\"Scenario2_Lagged\",\n",
        "        dep_var_name=dv,\n",
        "        dataf=df.copy(),\n",
        "        intangible_set=lagged_intangibles,\n",
        "        current_dummy_controls=lagged_dummy_controls,\n",
        "        run_ldv_on_model_c=False,\n",
        "        specific_models_to_run=['A', 'B', 'C'] # Explicitly run all three\n",
        "    )\n",
        "\n",
        "# Scenario 2 - Model C with LDV (Robustness Check - ONLY RUN MODEL C)\n",
        "print(\"\\n\\n\" + \"*\"*80)\n",
        "print(\" EXECUTING OLS SCENARIO 2: MODEL C with LDV (ROBUSTNESS) \".center(80, \"*\"))\n",
        "print(\"*\"*80 + \"\\n\")\n",
        "all_fitted_models_scenario2_model_c_ldv = {} # Renamed for clarity\n",
        "for dv in dependent_vars_list:\n",
        "    all_fitted_models_scenario2_model_c_ldv[dv] = run_ols_scenario(\n",
        "        scenario_label=\"Scenario2_Lagged_ModelC_LDV\", # More specific label\n",
        "        dep_var_name=dv,\n",
        "        dataf=df.copy(),\n",
        "        intangible_set=lagged_intangibles,\n",
        "        current_dummy_controls=lagged_dummy_controls,\n",
        "        run_ldv_on_model_c=True,\n",
        "        specific_models_to_run=['C'] # KEY CHANGE: Only run Model C\n",
        "    )\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*30 + \" END OF OLS ANALYSIS \" + \"=\"*30)"
      ],
      "metadata": {
        "id": "dHHbm5zEFQZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}